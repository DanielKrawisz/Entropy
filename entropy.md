# Entropy

## Conceptual Issues

### Irreversibility

Entropy is a theoretical quantity that explains irreversibility in physics. Known laws of physics are reversible, which means some physical process would be equally valid both forwards or backwards in time. (For everyday phsyics, this statement is exactly true, whereas for some processes in high energy physics the time-reversed version of a process would also have to be mirror-image reversed as well in order to be valid, whereas for everyday physics, mirror image reversal is also an exact symmetry.) However, in ordinary experience, many unexceptional processes would be quite exceptional if they were reversed. No one, for example, has ever experienced a convergence of sound waves into a puddle of liquid and pieces of glass sufficient to project them into the air and together in his hand as an unbroken cup of hot tea but many people have experienced the time-reversed version of such a process.

Thus, we observe that physics is irreversible on an everday scale but in all controlled conditions designed to experiment with fundamental processes, physics is reversible. The resolution of this apparent paradox follows from a statistical understanding of the physics in our everday experience. Not knowing the precise interactions of all particles around us, we have no recourse but to treat the evolution of our everyday environment as random. The relevant features of our experience do not depend on the exact position of every particle in our environment. Rather, many possible configurations of matter would result in indistinguishable experiences. 

If our world is indeed random, then it follows that a state with more states that are similar to it is *more likely* than one with fewer. This conclusion explains irreversibility in everyday physics. An irreversible process in everyday experience is not literally irreversible, but is a transition from less likely state to a more likely state. Indeed, different configurations of matter might be indistinguishable from vastly different numbers of similar configurations, which means that some possibilities are so unlikely that they never happen. To put it concretely, among all states in which there is a puddle and broken teacup on the floor, there are vastly fewer similar states with perfectly prepared tea than without, whereas among all states with perfectly prepared tea there are relatively many similar states with a puddle and a broken cup. 

A configuration that is similar to fewer other states is considered to be low entropy and one that is similar to many others is high entropy. Evolution from low to high entropy is likely, whereas high to low is unlikely, though not impossible. That is all that entropy is and all that it implies. 

### Entropy as Disorder

Entropy is often unhelpfully characterized as an expression of the disorder of a system. This analogy certainly raises more questions than it answers. 

While it is true that a disordered state has more entropy than an ordered state, entropy cannot be said to be a measure of disorder because disorder only exists relative to some kind of order. Imagine, for example, that there was no such thing as an unbroken tea cup. Would we then say that broken glass represented a state of disorder? We would not because there would be no other way that it could be. Such a state would still have entropy and its entropy would not be significantly altered by removing all states involving unbroken tea cups because the number of such states would be overwhelmed by the number of other possible configurations of glass.

Thus, if we were to say that a system had a certain amount of entropy, this value could not be said to correspond to some amount of disorder in the system, whatever that means. Rather, the system would be less ordered than some other configuration of the system with less entropy. If you wanted to say that the system was disordered, you ought to show that an ordered state was actually possible. 

Entropy as an explanation for irreversibily makes a lot more sense than entropy as disorder. However, with an accurate understanding of entropy as disorder, we can say that a tendency towards disorder is the explanatiion for irreversibility in physics. 

### Measurability of Entropy

Entropy is not a measurable quantity but rather a theoretical one that we ascribe to systems based on our theoretical knowledge of them. If we were to ascribe a certain value $S$ of entropy to a system, this value would be an inference based on our idea about how many states a sytem can actually have for given values of energy and possibly other observable quantities. Suppose, for example, that you cleaned your room. It would be possible to estimate the change in entropy of the room by attempting to enumerate the number of clean states in relation to messy states, but this would not be a measurement. It would be a theoretical result that would depend on an accurate understanding of all possible configurations of matter in the room. 

Differences in entropy are sometimes measurable. In particular, observations about how the temperature of a system changes in response to changes in heat energy tell us about changes in entropy. It is possible that other experiements can tell us about other entropy differences, but it is in the context of heat and temperature that entropy is best understood.  

In real systems, there can always be hidden order that is not understood. Think of someone you probably know whose desk alawys appears to be in a state of chaos but who never has trouble finding where anything is. His desk is therefore more ordered than it appears. If you rearranged his desk, he would not be able to find anything. Thus, two configurations that are apparently similar are actually not similar, and can be distinguished by asking your friend if his desk is messy. Thus, some idea about how much entropy a system has can change as a result of a deeper understanding of the system resulting from some new scientific investigation of it. 

### The Second Law of Thermodynamics

If entropy is not disorder and is not measureable, what does the second law of thermodynamics mean? This law states 

2. **In a closed system, the entropy never decreases.**

I said above that a more disordered state has more entropy than an ordered state, so this law does mean that disorder always increases. However, given that entropy is not measurable, this law is certainly of a different character than the law of gravity or even the first law of thermodynamics, which states 

1. **Energy is conserved.**

These laws can be tested empirically, whereas the second cannot. If entropy is an explanation for disorder in phsyics, the second law is a tautology becasue if entropy decreased, that would indicate that some process had occurred which can only happen backwards in time. 

When entropy was first described, physicists inferred its existence but did not understand what it was. Plank, for example, believed that entropy was a fundamental quantity like energy that would eventually be observed directly. However, he was disappointed when Boltzman was able to explain it as emerging out of statistical mechanics because he wanted to be working on fundamental laws, not emergent quantities. Before Boltzman's work, physicists did not understand entropy as relating to disorder. Instead, entropy was understood as a quantity whose increase expressed irreversibility in physics. 

The second law of thermodynamics is certainly a deep and important principle that is at the heart of everything in our experience, but it is not a law of physics in the ordinary sense. It is closer in character to something like praxeology than to other laws of physics because it has to do with ideal things like value that we ascribe to the world but cannot observe. 

## Entropy as the Foundation of Thermodynamics

### Temperature vs. Heat

Early researches into thermodynammics concerned questions about the nature of hot and cold. Since antiquity, some people had understood that a fluid changes volume in response to heat. This observation was the basis for the first thermomenters. Originally, *degree* referred to an arbitrary line that was drawn on the device because no one knew what temperature was actually measuring, although it was understood that different thermometers could be calibrated with respect to one another and thus were all measuring the same thing. 

But what was that thing? Are hot and cold different substances, or is cold merely the absence of hot? Is heat a substance in its own right or is it a kind of motion? These questions were resolved with an experiment that showed that water could be brought to a boil with friction alone. In other words, an unlimited quantity of heat could be produced from motion, and thus it appeared that heat merely was motion. 

Understanding the connection between heat and motion made it possible to produce heat in specific amounts whose effects could be tested on different kinds of matter. This made it obvious that temperature did not measure heat. Instead, every substance has its own heat capacity, which is an amount of heat required to raise a given mass of the substance one degree of temperature. 

But if temperature is not a measure of heat, what is it measuring? 

### Temperature in Terms of Entropy

The zeroth law of thermodynamics, which was added later, states 

0. **Bodies that are in thermal equilibrium with one another have the same temperature.**

Thus, temperature is measured by allowing a thermometer to come into equilibrium with the substance being tested. At equilibrium, the substance is giving as much energy from its interaction with the thermometer as the thermometer is giving back to it. This information is enough to guess at the relationship between temperature and entropy. 

Two bodies which are at thermal equilibrium with one another should not be able to increase their total entropy by transferring energy from one body to another. Thus the number of states in each body must be increasing with regard to increases in energy at the same rate. If the number of states in one body is increasing faster with regard to increases in energy than the other, than we would expect energy to move into that body from the other and we would say that it has a lower temperature. 

Thus for entropy $S$, energy $E$, and temperature $T$, we might guess that 

$$ \frac{1}{T} = \frac{d S}{d E} $$

which turns out to be correct. 

Certain ideal substances could have a formula for temperature asscribed to them. For ideal gasses, temperature is proportional to the average kinetic energy of a particle in the gas. This formula explains the heat capacity of gasses in terms of their molecular composition. In particular, some gasses store potential energy as well as kinetic energy. In general, gasses with more complex molecules will store more average potential energy than gasses with less complex molecules. Even though real gasses are not ideal, the ideal gas analysis explains why carbon dioxide is a stronger greenhouse gas than oxygen, and why methane is stronger than carbon dioxide. Stronger greenhouse gasses store more energy at a given temperature than weaker gasses, and are thus at a higher temperature when they radiate the same amount energy into space that they absorb from the sun. 

## Thermal Equilibrium 

In theoretical physics today, entropy is considered to be a more fundamental idea than temperature. Originally, temperature was defined in terms of thermometers and entropy was defined as in terms of temperature via the integral version of the expression above. 

Nowadays, temperature is defined in terms of entropy. However, there is a big difference between temperature as measured in a thermometer and temperature as a function of entropy. The difference is that a real thermometer will always measure a temperature whereas the theoretical definition only exists in thermal equilibrium. 

The idealization of thermal equilibrium is the reason I say that entropy cannot be measured in real systems. One can measure temperature but one cannot make inferences about entropy without knowing that the measurement is taken at thermal equilibrium. But we cannot know that we are at thermal equilibrium without an exact knowledge of the system in question, which we only have of theoretical systems, not real ones. You can of course watch a system and observe that it has not apparently changed in a while, but there could always be some very slow reaction going on that is not evident over your observational time scale. 

For exmaple, imagine a system of two gasses that were mixed together but which interacted only weakly. One gas could be hot and the other cold. A thermometer would measure a temperature if it was 

For example, suppose that the proton decays, as many theoretical physicists suspect. That would mean that there are unknown states that are not being accounted for that are not being occupied by any energy added to some system consisting of ordinary matter. Any measurement of the temperature of ordinary matter would have to be less than the theoretical temperature defined in terms of entropy. 

In the example above, the measured temperature would give us a good idea of the entropy. 

## The Third Law of Thermodynamics

The third law of thermodynamics states 

3. **As the temperature approaches absolute zero, the entropy approaches a constant value.**

This can be understood by transforming the differential expression above into an integral expression:

$$ E = \int T d S + C $$ 

The constant at the end of the integral expression is necessary to set the entropy at absolute zero, which is different for different materials. Two different systems at absolute zero may have different entropies because a system may have one or more lowest energy states. If there is one lowest state then the entropy at absolute zero ought to be zero because there is no ambiguity about which state it is in, whereas if there are more than one, it ought to be greater than zero. 

## Entropy and Life 

Many processes, an in particular the evolution of life, appear to contradict the idea that there is always a tendency toward more disorder. 

Consider a lion. The lion is a very ordered system. Or is it? The whole lion system is not merely the body of the lion but also the zebras that it eats. Without them, the lion system could not stay ordered. Thus, a lion maintains its own order but destroys the order of many zebras. In the net, the lion system maintains order inside its own body by creating a great deal more disorder outside its body. All life works this way. 

If you consider that all life is driven by the fire in the sun, the total entropy being created there is enormous in comparison to anything going on on the Earth. 

Statistical Physics of self-replication

## Entropy and Quantum Gravity

Quantum information conjecture
